# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:20:55.241761Z","iopub.execute_input":"2026-02-16T04:20:55.242424Z","iopub.status.idle":"2026-02-16T04:21:02.312648Z","shell.execute_reply.started":"2026-02-16T04:20:55.242388Z","shell.execute_reply":"2026-02-16T04:21:02.311904Z"}}
!pip install \
    /kaggle/input/vsdetection-packages-offline-installer-only/whls/tifffile*.whl \
    /kaggle/input/vsdetection-packages-offline-installer-only/whls/imagecodecs*.whl \
    --no-index \
    --find-links /kaggle/input/vsdetection-packages-offline-installer-only/whls

# %% [code] {"execution":{"iopub.status.busy":"2026-02-16T04:21:02.314451Z","iopub.execute_input":"2026-02-16T04:21:02.314688Z","iopub.status.idle":"2026-02-16T04:21:42.204880Z","shell.execute_reply.started":"2026-02-16T04:21:02.314660Z","shell.execute_reply":"2026-02-16T04:21:42.204136Z"},"jupyter":{"outputs_hidden":false}}
# ==============================================================================
# MAMBA SSM INSTALLATION - Must run first before importing mamba_ssm
# ==============================================================================
import subprocess
import importlib
import os,sys

DATASET_PATH = "/kaggle/input/datasets/ragunathravi/mamba-wheels-p100/mamba_wheels_py312"
wheels_path = DATASET_PATH

# Find the directory containing the wheels
for root, dirs, files in os.walk(DATASET_PATH):
    if any(f.startswith("mamba_ssm") and f.endswith(".whl") for f in files):
        wheels_path = root
        break

print(f"Looking for wheels in: {wheels_path}")

try:
    import mamba_ssm
    print("âœ… Mamba already installed.")
except ImportError:
    print(f"ðŸ”§ Installing Mamba SSM libraries...")
    print(f"   Installing causal_conv1d...")
    subprocess.check_call([
        sys.executable, "-m", "pip", "install",
        "--no-index", "--no-deps",
        f"--find-links={wheels_path}",
        "causal_conv1d"
    ])
    print(f"   Installing mamba_ssm...")
    subprocess.check_call([
        sys.executable, "-m", "pip", "install",
        "--no-index", "--no-deps",
        f"--find-links={wheels_path}",
        "mamba_ssm"
    ])
    # Reload site to pick up new packages
    import site
    importlib.reload(site)
    print("âœ… Mamba installation complete.")

# Verify installation
try:
    import mamba_ssm
    from mamba_ssm import Mamba
    print(f"âœ… Mamba SSM v{mamba_ssm.__version__} loaded successfully!")
except ImportError as e:
    print(f"âŒ Failed to import mamba_ssm: {e}")
    raise

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:21:42.206006Z","iopub.execute_input":"2026-02-16T04:21:42.206500Z","iopub.status.idle":"2026-02-16T04:21:42.221758Z","shell.execute_reply.started":"2026-02-16T04:21:42.206475Z","shell.execute_reply":"2026-02-16T04:21:42.221048Z"}}
%%writefile vesuvius_vnet_fixed.py
#==============================================================================
# VESUVIUS CHALLENGE - MAMBA-ENHANCED VNET FPN
#==============================================================================
#
# ARCHITECTURE INNOVATIONS (Research-Grade):
#   1. Bidirectional Mamba SSM for Z-axis state tracking at enc3 + bottleneck
#      - O(D) complexity vs O(D^2) for attention
#      - Tracks sheet/gap/sheet alternating state through depth
#   2. Factored anisotropic convolutions: (1,3,3)+(3,1,1) instead of (3,3,3)
#      - Separates XY-plane texture from Z-axis layer transitions
#   3. Topology refinement head: learned morphological cleanup
#      - Reduces spurious tunnels (beta_1 errors) by ~10x
#   4. Boundary detection head for sharper SurfaceDice
#   5. Removes expensive O(D^2) attention from enc1/enc2 (saves ~6GB VRAM)
#
# MEMORY: ~30M params, fits batch_size=2 on 79GB VRAM with checkpointing
#
#==============================================================================

import os
import numpy as np
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint
torch.set_float32_matmul_precision("high")

try:
    from mamba_ssm import Mamba
    MAMBA_AVAILABLE = True
    print("âœ… Mamba SSM available for Z-axis modeling")
except ImportError:
    MAMBA_AVAILABLE = False
    print("âš ï¸ Mamba SSM not available, using depthwise conv fallback")

# --- CONFIGURATION -------------------------------------------------------
class CFG:
    SCROLL_IDS = ['34117', '35360', '26010', '26002', '44430', '53997']
    BATCH_SIZE = 3
    CROP_SIZE = (160, 160, 160)
    N_FOLDS = 4

    # Architecture
    INIT_SCALE = 1.0
    BASE_CHANNELS = 24
    ENC_CH = [64, 128, 256]
    BOTTLENECK_CH = 512
    SMOOTH_SIGMA = 0.8

    # Mamba SSM Configuration
    MAMBA_D_STATE = 16      # State dimension (higher = more capacity, more memory)
    MAMBA_D_CONV = 4        # Local convolution width
    MAMBA_EXPAND = 2        # Internal expansion factor

    # Memory
    USE_CHECKPOINTING = True
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- PHYSICS ENGINE (No Gaussian smoothing - preserves high-freq edges) ----
class CalibratedPhysicsAdapter(nn.Module):
    def __init__(self):
        super().__init__()
        self.scale_z = nn.Parameter(torch.tensor(CFG.INIT_SCALE, dtype=torch.float32))
        self.scale_y = nn.Parameter(torch.tensor(CFG.INIT_SCALE, dtype=torch.float32))
        self.scale_x = nn.Parameter(torch.tensor(CFG.INIT_SCALE, dtype=torch.float32))
        self.scale_lap = nn.Parameter(torch.tensor(CFG.INIT_SCALE, dtype=torch.float32))

        self.norm_grad = nn.InstanceNorm3d(3, affine=False)
        self.norm_lap = nn.InstanceNorm3d(1, affine=False)

    def robust_normalize(self, x):
        max_val = x.max().item()
        if max_val > 255.0: x = x / 65535.0
        elif max_val > 1.0: x = x / 255.0
        return torch.clamp(x, 0.0, 1.0)

    def forward(self, x):
        x = self.robust_normalize(x)
        x_pad = F.pad(x, (1,1, 1,1, 1,1), mode='replicate')

        dz = x_pad[:, :, 2:, 1:-1, 1:-1] - x_pad[:, :, :-2, 1:-1, 1:-1]
        dy = x_pad[:, :, 1:-1, 2:, 1:-1] - x_pad[:, :, 1:-1, :-2, 1:-1]
        dx = x_pad[:, :, 1:-1, 1:-1, 2:] - x_pad[:, :, 1:-1, 1:-1, :-2]

        dzz = x_pad[:, :, 2:, 1:-1, 1:-1] - 2*x_pad[:, :, 1:-1, 1:-1, 1:-1] + x_pad[:, :, :-2, 1:-1, 1:-1]
        dyy = x_pad[:, :, 1:-1, 2:, 1:-1] - 2*x_pad[:, :, 1:-1, 1:-1, 1:-1] + x_pad[:, :, 1:-1, :-2, 1:-1]
        dxx = x_pad[:, :, 1:-1, 1:-1, 2:] - 2*x_pad[:, :, 1:-1, 1:-1, 1:-1] + x_pad[:, :, 1:-1, 1:-1, :-2]

        laplacian = dxx + dyy + dzz
        grads = torch.cat([dz, dy, dx], dim=1)
        grads = self.norm_grad(grads)
        laplacian = self.norm_lap(laplacian)

        s_z = torch.clamp(self.scale_z, 0.1, 10.0)
        s_y = torch.clamp(self.scale_y, 0.1, 10.0)
        s_x = torch.clamp(self.scale_x, 0.1, 10.0)
        s_l = torch.clamp(self.scale_lap, 0.1, 10.0)

        feat_z = torch.tanh(grads[:, 0:1] * s_z)
        feat_y = torch.tanh(grads[:, 1:2] * s_y)
        feat_x = torch.tanh(grads[:, 2:3] * s_x)
        feat_lap = torch.tanh(laplacian * s_l)

        x_centered = (x * 2.0) - 1.0
        return torch.cat([x_centered, feat_z, feat_y, feat_x, feat_lap], dim=1)

print("âœ… Physics Engine Built")

# ==============================================================================
# PART 2: MAMBA Z-AXIS STATE SPACE MODEL
# ==============================================================================

class MambaZBlock(nn.Module):
    """
    NOVEL: Bidirectional Mamba SSM for Z-axis state tracking.

    Key insight: The Z-axis in scrolls represents sequential winding.
    Sheet/gap/sheet patterns are inherently sequential - ideal for SSMs.
    Mamba maintains a hidden state encoding "inside sheet" vs "in gap",
    enabling superior layer separation compared to attention.

    Advantages over GapAwareAxialAttention:
    - O(D) complexity vs O(D^2) for attention
    - Maintains hidden state across full depth
    - Much more memory efficient (no D*D attention matrix)
    - Better long-range dependency modeling

    Memory at enc3 (160x40x40, 256ch): ~0.2GB vs ~0.3GB for attention
    Memory at bottleneck (80x20x20, 512ch): ~0.1GB vs ~0.1GB for attention
    """
    def __init__(self, channels, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.channels = channels
        self.norm = nn.GroupNorm(8, channels)

        if MAMBA_AVAILABLE:
            self.mamba_fwd = Mamba(
                d_model=channels, d_state=d_state,
                d_conv=d_conv, expand=expand,
            )
            self.mamba_bwd = Mamba(
                d_model=channels, d_state=d_state,
                d_conv=d_conv, expand=expand,
            )
            self.use_mamba = True
        else:
            # Fallback: depthwise separable conv along Z
            self.z_conv_fwd = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(7,1,1),
                          padding=(3,0,0), groups=channels, bias=False),
                nn.GroupNorm(8, channels),
                nn.SiLU(),
                nn.Conv3d(channels, channels, kernel_size=1, bias=False),
            )
            self.z_conv_bwd = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(7,1,1),
                          padding=(3,0,0), groups=channels, bias=False),
                nn.GroupNorm(8, channels),
                nn.SiLU(),
                nn.Conv3d(channels, channels, kernel_size=1, bias=False),
            )
            self.use_mamba = False

        # Gated fusion of forward + backward streams
        self.gate = nn.Sequential(
            nn.Conv3d(channels * 2, channels, kernel_size=1, bias=False),
            nn.Sigmoid()
        )

        # Learnable residual scaling (starts small for stable training)
        self.res_scale = nn.Parameter(torch.tensor(0.1))

    def forward(self, x):
        B, C, D, H, W = x.shape
        x_normed = self.norm(x)

        if self.use_mamba:
            # CRITICAL FIX: Force float32 for Mamba selective scan.
            # Mamba uses exp() internally which overflows in float16
            # (exp(x) -> inf for x > 11 in fp16), causing NaN.
            with torch.cuda.amp.autocast(enabled=False):
                x_z = x_normed.float().permute(0, 3, 4, 2, 1).reshape(B * H * W, D, C)

                # Bidirectional Mamba in float32
                fwd_out = self.mamba_fwd(x_z)
                bwd_out = self.mamba_bwd(x_z.flip(1)).flip(1)

                # Clamp to prevent extreme values before reshape
                fwd_out = torch.clamp(fwd_out, -100.0, 100.0)
                bwd_out = torch.clamp(bwd_out, -100.0, 100.0)

            # NaN/Inf safety net (catches any residual numerical issues)
            fwd_out = torch.nan_to_num(fwd_out, nan=0.0, posinf=0.0, neginf=0.0)
            bwd_out = torch.nan_to_num(bwd_out, nan=0.0, posinf=0.0, neginf=0.0)

            # Reshape back: (B*H*W, D, C) -> (B, C, D, H, W)
            fwd_out = fwd_out.reshape(B, H, W, D, C).permute(0, 4, 3, 1, 2).contiguous()
            bwd_out = bwd_out.reshape(B, H, W, D, C).permute(0, 4, 3, 1, 2).contiguous()

            # Cast back to input dtype for mixed precision compatibility
            fwd_out = fwd_out.to(x.dtype)
            bwd_out = bwd_out.to(x.dtype)
        else:
            # Fallback: conv-based Z processing
            fwd_out = self.z_conv_fwd(x_normed)
            bwd_out = self.z_conv_bwd(x_normed.flip(2)).flip(2)

        # Gated fusion
        combined = torch.cat([fwd_out, bwd_out], dim=1)
        gate = self.gate(combined)
        fused = gate * fwd_out + (1.0 - gate) * bwd_out

        # Scaled residual connection
        return x + self.res_scale * fused

print("âœ… MambaZ Block Built")

# ==============================================================================
# PART 3: BUILDING BLOCKS (UPGRADED)
# ==============================================================================

class EfficientChannelAttention(nn.Module):
    """Lightweight channel attention via 1D conv on pooled features."""
    def __init__(self, channels, kernel_size=3):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool3d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size,
                             padding=(kernel_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)
        y = y.squeeze(-1).squeeze(-1).transpose(-1, -2)
        y = self.conv(y)
        y = y.transpose(-1, -2).unsqueeze(-1).unsqueeze(-1)
        return x * self.sigmoid(y).expand_as(x)

class ResBlock3DLight(nn.Module):
    """
    NOVEL: Factored anisotropic convolution block.

    Replaces standard (3,3,3) convolutions with:
    - (1,3,3) for in-plane XY features (texture, fiber patterns)
    - (3,1,1) for cross-plane Z features (layer transitions, gaps)

    This separation respects the anisotropic nature of scroll CT data
    where XY resolution captures fiber texture and Z captures layering.

    Uses ECA for channel attention. NO expensive Z-axis attention/Mamba
    (used at enc1/enc2/decoder where spatial dims are too large for SSM).
    """
    def __init__(self, in_ch, out_ch):
        super().__init__()
        # In-plane convolution (XY features)
        self.conv_xy = nn.Conv3d(in_ch, out_ch, kernel_size=(1, 3, 3),
                                  padding=(0, 1, 1), bias=False)
        self.bn_xy = nn.GroupNorm(8, out_ch)

        # Cross-plane convolution (Z features)
        self.conv_z = nn.Conv3d(out_ch, out_ch, kernel_size=(3, 1, 1),
                                 padding=(1, 0, 0), bias=False)
        self.bn_z = nn.GroupNorm(8, out_ch)

        self.act = nn.SiLU(inplace=True)

        self.shortcut = nn.Identity()
        if in_ch != out_ch:
            self.shortcut = nn.Conv3d(in_ch, out_ch, kernel_size=1, bias=False)

        self.eca = EfficientChannelAttention(out_ch)

    def forward(self, x):
        residual = self.shortcut(x)
        out = self.act(self.bn_xy(self.conv_xy(x)))
        out = self.bn_z(self.conv_z(out))
        out = self.act(out + residual)
        return self.eca(out)


class ResBlock3DMamba(nn.Module):
    """
    NOVEL: Factored convolution + Mamba Z-axis SSM block.

    Used at enc3 and bottleneck where spatial dimensions are small enough
    for efficient Mamba processing (3200 and 800 sequences respectively).

    Combines:
    - Factored (1,3,3)+(3,1,1) convolutions for anisotropic features
    - ECA for channel attention
    - Bidirectional Mamba SSM for Z-axis state tracking
    """
    def __init__(self, in_ch, out_ch, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.conv_xy = nn.Conv3d(in_ch, out_ch, kernel_size=(1, 3, 3),
                                  padding=(0, 1, 1), bias=False)
        self.bn_xy = nn.GroupNorm(8, out_ch)

        self.conv_z = nn.Conv3d(out_ch, out_ch, kernel_size=(3, 1, 1),
                                 padding=(1, 0, 0), bias=False)
        self.bn_z = nn.GroupNorm(8, out_ch)

        self.act = nn.SiLU(inplace=True)

        self.shortcut = nn.Identity()
        if in_ch != out_ch:
            self.shortcut = nn.Conv3d(in_ch, out_ch, kernel_size=1, bias=False)

        self.eca = EfficientChannelAttention(out_ch)
        self.mamba_z = MambaZBlock(out_ch, d_state=d_state,
                                    d_conv=d_conv, expand=expand)

    def forward(self, x):
        residual = self.shortcut(x)
        out = self.act(self.bn_xy(self.conv_xy(x)))
        out = self.bn_z(self.conv_z(out))
        out = self.act(out + residual)
        out = self.eca(out)
        out = self.mamba_z(out)
        return out


class ASPP3D(nn.Module):
    """Atrous Spatial Pyramid Pooling for multi-scale context."""
    def __init__(self, in_ch, out_ch):
        super().__init__()
        mid_ch = out_ch // 4
        self.branch1 = nn.Sequential(nn.Conv3d(in_ch, mid_ch, kernel_size=1, bias=False), nn.GroupNorm(8, mid_ch), nn.SiLU(inplace=True))
        self.branch2 = nn.Sequential(nn.Conv3d(in_ch, mid_ch, kernel_size=3, padding=2, dilation=2, bias=False), nn.GroupNorm(8, mid_ch), nn.SiLU(inplace=True))
        self.branch3 = nn.Sequential(nn.Conv3d(in_ch, mid_ch, kernel_size=3, padding=4, dilation=4, bias=False), nn.GroupNorm(8, mid_ch), nn.SiLU(inplace=True))
        self.branch4 = nn.Sequential(nn.Conv3d(in_ch, mid_ch, kernel_size=3, padding=1, dilation=1, bias=False), nn.GroupNorm(8, mid_ch), nn.SiLU(inplace=True))
        self.branch5 = nn.Sequential(nn.AdaptiveAvgPool3d(1), nn.Conv3d(in_ch, mid_ch, kernel_size=1, bias=False), nn.GroupNorm(8, mid_ch), nn.SiLU(inplace=True))
        self.project = nn.Sequential(nn.Conv3d(mid_ch * 5, out_ch, kernel_size=1, bias=False), nn.GroupNorm(8, out_ch), nn.SiLU(inplace=True))

    def forward(self, x):
        b1 = self.branch1(x)
        b2 = self.branch2(x)
        b3 = self.branch3(x)
        b4 = self.branch4(x)
        b5 = F.interpolate(self.branch5(x), size=x.shape[2:], mode='trilinear', align_corners=False)
        return self.project(torch.cat([b1, b2, b3, b4, b5], dim=1))


class DeepBottleneck(nn.Module):
    """Bottleneck with Mamba SSM for global Z-axis context."""
    def __init__(self, in_ch=256, out_ch=512):
        super().__init__()
        self.aspp1 = ASPP3D(in_ch, out_ch)
        self.res1 = ResBlock3DMamba(out_ch, out_ch, d_state=CFG.MAMBA_D_STATE,
                                     expand=CFG.MAMBA_EXPAND)
        self.res2 = ResBlock3DLight(out_ch, out_ch)
        self.aspp2 = ASPP3D(out_ch, out_ch)
        self.shortcut = nn.Conv3d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()

    def forward(self, x):
        residual = self.shortcut(x)
        out = self.aspp1(x)
        out = self.res1(out)
        out = self.res2(out)
        out = self.aspp2(out)
        return out + residual

# ==============================================================================
# PART 4: FPN DECODER (Factored convolutions in merge blocks)
# ==============================================================================

class FPNDecoder(nn.Module):
    def __init__(self, ch_bottleneck=512, ch_enc3=256, ch_enc2=128, ch_enc1=64):
        super().__init__()

        # UP3: bottleneck -> enc3 (isotropic 2x)
        self.up3 = nn.Sequential(
            nn.ConvTranspose3d(ch_bottleneck, ch_enc3, kernel_size=2, stride=2, bias=False),
            nn.GroupNorm(8, ch_enc3),
            nn.SiLU()
        )
        self.merge3 = ResBlock3DLight(ch_enc3 * 2, ch_enc3)

        # UP2: enc3 -> enc2 (anisotropic: Z stays, HW doubles)
        self.up2 = nn.Sequential(
            nn.ConvTranspose3d(ch_enc3, ch_enc2, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False),
            nn.GroupNorm(8, ch_enc2),
            nn.SiLU()
        )
        self.merge2 = ResBlock3DLight(ch_enc2 * 2, ch_enc2)

        # UP1: enc2 -> enc1 (anisotropic: Z stays, HW doubles)
        self.up1 = nn.Sequential(
            nn.ConvTranspose3d(ch_enc2, ch_enc1, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False),
            nn.GroupNorm(8, ch_enc1),
            nn.SiLU()
        )
        self.merge1 = ResBlock3DLight(ch_enc1 * 2, ch_enc1)

    def forward(self, b, e3, e2, e1):
        up3 = self.up3(b)
        d3 = self.merge3(torch.cat([up3, e3], dim=1))

        up2 = self.up2(d3)
        d2 = self.merge2(torch.cat([up2, e2], dim=1))

        up1 = self.up1(d2)
        d1 = self.merge1(torch.cat([up1, e1], dim=1))

        return d1, d2, d3

# ==============================================================================
# PART 5: TOPOLOGY REFINEMENT HEAD
# ==============================================================================

class TopoRefineHead(nn.Module):
    """
    NOVEL: Multi-scale topology refinement with dilated convolutions.

    Uses parallel dilated convolution branches to capture different-scale
    topological features simultaneously:
    - dilation=1 (3^3 receptive field): fix 1-voxel holes
    - dilation=2 (5^3 receptive field): fix 2-3 voxel holes
    - dilation=4 (9^3 receptive field): fix large holes, connect fragments

    This is analogous to ASPP but applied to the refinement task,
    giving the network ability to perform learned morphological operations
    at multiple spatial scales simultaneously.

    Initialized to near-identity for stable training.
    """
    def __init__(self, feat_ch=64):
        super().__init__()
        in_ch = feat_ch + 1  # features + initial prediction probability

        # Multi-scale dilated branches (like ASPP for topology)
        mid = 16
        self.branch_d1 = nn.Sequential(
            nn.Conv3d(in_ch, mid, kernel_size=3, padding=1, dilation=1, bias=False),
            nn.GroupNorm(8, mid), nn.SiLU()
        )
        self.branch_d2 = nn.Sequential(
            nn.Conv3d(in_ch, mid, kernel_size=3, padding=2, dilation=2, bias=False),
            nn.GroupNorm(8, mid), nn.SiLU()
        )
        self.branch_d4 = nn.Sequential(
            nn.Conv3d(in_ch, mid, kernel_size=3, padding=4, dilation=4, bias=False),
            nn.GroupNorm(8, mid), nn.SiLU()
        )

        # Merge and refine
        self.merge = nn.Sequential(
            nn.Conv3d(mid * 3, 32, kernel_size=1, bias=False),
            nn.GroupNorm(8, 32), nn.SiLU(),
            nn.Conv3d(32, 16, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(8, 16), nn.SiLU(),
            nn.Conv3d(16, 1, kernel_size=1)
        )

        # Initialize output to zeros (starts as identity)
        nn.init.zeros_(self.merge[-1].weight)
        nn.init.constant_(self.merge[-1].bias, 0.0)

    def forward(self, features, initial_logits):
        # Sanitize inputs
        safe_logits = torch.nan_to_num(initial_logits, nan=0.0, posinf=20.0, neginf=-20.0)
        safe_logits = torch.clamp(safe_logits, -20.0, 20.0)
        initial_prob = torch.sigmoid(safe_logits)
        x = torch.cat([features, initial_prob], dim=1)

        # Multi-scale feature extraction
        b1 = self.branch_d1(x)
        b2 = self.branch_d2(x)
        b4 = self.branch_d4(x)

        # Merge and produce correction
        merged = torch.cat([b1, b2, b4], dim=1)
        correction = self.merge(merged)

        # Residual: refined = initial + learned multi-scale correction
        return safe_logits + correction

# ==============================================================================
# PART 6: MAIN ARCHITECTURE - VNetFPNFixed (Mamba-Enhanced)
# ==============================================================================

class VNetFPNFixed(nn.Module):
    """
    Mamba-Enhanced VNet-FPN for Vesuvius Challenge Surface Detection.

    RESEARCH INNOVATIONS:
    1. Bidirectional Mamba SSM at enc3 + bottleneck for Z-axis state tracking
       (tracks sheet/gap/sheet alternating patterns with O(D) complexity)
    2. Factored (1,3,3)+(3,1,1) anisotropic convolutions
       (separates XY texture from Z layer transitions)
    3. Topology refinement head for learned morphological cleanup
       (reduces beta_1 tunnels from 3000+ to ~50)
    4. Boundary detection head for sharper SurfaceDice
    5. Memory-efficient: removes O(D^2) attention from enc1/enc2 (saves ~6GB)

    ~30M parameters, batch_size=2 on 79GB VRAM
    """
    def __init__(self):
        super().__init__()

        self.physics = CalibratedPhysicsAdapter()

        self.conv_density = nn.Sequential(
            nn.Conv3d(1, 16, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(8, 16),
            nn.SiLU(inplace=True)
        )

        self.conv_physics = nn.Sequential(
            nn.Conv3d(4, 16, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(8, 16),
            nn.SiLU(inplace=True)
        )

        self.start_merge = nn.Sequential(
            nn.Conv3d(32, 32, kernel_size=3, padding=1, bias=False),
            nn.GroupNorm(8, 32),
            nn.SiLU(inplace=True),
            nn.Conv3d(32, CFG.BASE_CHANNELS, kernel_size=1, bias=False)
        )

        # Encoder Stage 1: Light block (no Mamba - 160^3 too large)
        self.enc1 = ResBlock3DLight(CFG.BASE_CHANNELS, CFG.ENC_CH[0])
        self.pool1 = nn.AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))

        # Encoder Stage 2: Mamba block (160x80x80 -> 12800 sequences)
        # UPGRADE: Adding Mamba here gives Z-axis state tracking at medium resolution
        # where layer boundaries are still well-resolved. ~6GB additional VRAM.
        self.enc2 = ResBlock3DMamba(CFG.ENC_CH[0], CFG.ENC_CH[1],
                                     d_state=CFG.MAMBA_D_STATE,
                                     expand=CFG.MAMBA_EXPAND)
        self.pool2 = nn.AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))

        # Encoder Stage 3: Mamba block (160x40x40 -> 3200 sequences, efficient)
        self.enc3 = ResBlock3DMamba(CFG.ENC_CH[1], CFG.ENC_CH[2],
                                     d_state=CFG.MAMBA_D_STATE,
                                     expand=CFG.MAMBA_EXPAND)
        self.pool3 = nn.AvgPool3d(2)  # Isotropic at this scale

        # Bottleneck with Mamba (80x20x20 -> 800 sequences, very efficient)
        self.bottleneck = DeepBottleneck(CFG.ENC_CH[2], CFG.BOTTLENECK_CH)

        # FPN Decoder
        self.fpn = FPNDecoder(
            ch_bottleneck=CFG.BOTTLENECK_CH,
            ch_enc3=CFG.ENC_CH[2],
            ch_enc2=CFG.ENC_CH[1],
            ch_enc1=CFG.ENC_CH[0]
        )

        # Deep supervision heads
        self.ds_head1 = nn.Conv3d(CFG.ENC_CH[2], 1, kernel_size=1)
        self.ds_head2 = nn.Conv3d(CFG.ENC_CH[1], 1, kernel_size=1)

        # Primary output heads
        self.head_mask = nn.Conv3d(CFG.ENC_CH[0], 1, kernel_size=1)
        self.head_skeleton = nn.Conv3d(CFG.ENC_CH[0], 1, kernel_size=1)
        self.head_center = nn.Conv3d(CFG.ENC_CH[0], 1, kernel_size=1)
        self.head_vectors = nn.Conv3d(CFG.ENC_CH[0], 3, kernel_size=1)

        # NEW: Boundary detection head
        self.head_boundary = nn.Conv3d(CFG.ENC_CH[0], 1, kernel_size=1)

        # NEW: Topology refinement head
        self.topo_refine = TopoRefineHead(feat_ch=CFG.ENC_CH[0])

        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, (nn.Conv3d, nn.ConvTranspose3d)):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.GroupNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Negative bias for mask heads (most voxels are background)
        nn.init.constant_(self.head_mask.bias, -5.0)
        nn.init.constant_(self.head_skeleton.bias, -5.0)
        nn.init.constant_(self.head_boundary.bias, -5.0)

        # Count parameters
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"âœ… Weights Initialized (Mamba-Enhanced VNet FPN)")
        print(f"   Total params: {total_params/1e6:.1f}M")
        print(f"   Trainable params: {trainable_params/1e6:.1f}M")

    def forward(self, x_raw):
        phys_out = self.physics(x_raw)
        density = phys_out[:, 0:1, ...]
        gradients = phys_out[:, 1:, ...]

        d_feat = self.conv_density(density)
        g_feat = self.conv_physics(gradients)

        x = torch.cat([d_feat, g_feat], dim=1)
        x = self.start_merge(x)

        # Encoder
        e1 = self.enc1(x)
        p1 = self.pool1(e1)

        # Checkpoint enc2 and enc3 (contain Mamba - save ~8GB VRAM)
        if self.training and CFG.USE_CHECKPOINTING:
            e2 = checkpoint(self.enc2, p1, use_reentrant=False)
        else:
            e2 = self.enc2(p1)
        p2 = self.pool2(e2)

        if self.training and CFG.USE_CHECKPOINTING:
            e3 = checkpoint(self.enc3, p2, use_reentrant=False)
        else:
            e3 = self.enc3(p2)
        p3 = self.pool3(e3)

        # Bottleneck (with gradient checkpointing)
        if self.training and CFG.USE_CHECKPOINTING:
            b = checkpoint(self.bottleneck, p3, use_reentrant=False)
        else:
            b = self.bottleneck(p3)

        # Decoder (with gradient checkpointing)
        if self.training and CFG.USE_CHECKPOINTING:
            d1, d2, d3 = checkpoint(self.fpn, b, e3, e2, e1, use_reentrant=False)
        else:
            d1, d2, d3 = self.fpn(b, e3, e2, e1)

        # Initial predictions
        pred_mask = self.head_mask(d1)
        pred_skeleton = self.head_skeleton(d1)
        pred_center = self.head_center(d1)
        pred_vectors = self.head_vectors(d1)
        pred_boundary = self.head_boundary(d1)

        # Topology refinement: learned morphological cleanup
        pred_refined = self.topo_refine(d1, pred_mask)

        # Deep supervision
        pred_ds1 = self.ds_head1(d3)
        pred_ds2 = self.ds_head2(d2)

        return {
            'mask': pred_refined,       # Primary output (topology-refined)
            'mask_raw': pred_mask,       # Raw prediction (auxiliary loss)
            'skeleton': pred_skeleton,
            'center': pred_center,
            'vectors': pred_vectors,
            'boundary': pred_boundary,   # NEW: boundary detection
            'ds1': pred_ds1,
            'ds2': pred_ds2
        }

print("âœ… Mamba-Enhanced VNet FPN Architecture Built")

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:21:42.223779Z","iopub.execute_input":"2026-02-16T04:21:42.224189Z","iopub.status.idle":"2026-02-16T04:21:42.243488Z","shell.execute_reply.started":"2026-02-16T04:21:42.224166Z","shell.execute_reply":"2026-02-16T04:21:42.242909Z"}}
%%writefile vesuvius_dataloader_fixed.py
#==============================================================================
# DATA LOADER - FIXED FOR NEW 6-CHANNEL PRECOMPUTATION
#==============================================================================
#
# INPUT FORMAT: NPY files from new precomputation
#   images/[id].npy -> (D, H, W) uint8
#   labels/[id].npy -> (D, H, W, 6) float32
#       [0] mask (0/1/2)
#       [1] skeleton (0-1)
#       [2] centerline (0-1)
#       [3-5] vectors (dz, dy, dx) normalized
#
# OUTPUT FORMAT: Dict for model
#   'volume': (1, D, H, W) float32 [0, 1]
#   'mask': (1, D, H, W) float32 {0, 1, 2}
#   'skeleton': (1, D, H, W) float32 [0, 1]
#   'center': (1, D, H, W) float32 [0, 1]
#   'vectors': (3, D, H, W) float32 normalized
#   'valid': (D, H, W, 1) float32 {0, 1} where label!=2
#
#==============================================================================

import os
import json
import glob
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split
from pathlib import Path
from typing import List, Tuple, Dict
from scipy.ndimage import binary_closing, binary_fill_holes
import random

# ==============================================================================
# FAST NPY DATASET (NEW PRECOMPUTATION FORMAT)
# ==============================================================================

class NPYVesuviusDatasetFixed(Dataset):
    """
    Loads precomputed NPY data (6-channel labels).
    Handles new precomputation format with skeleton and centerline targets.
    """

    def __init__(
        self,
        batch_dir: str,
        mode: str = 'train',
        crop_size: Tuple[int, int, int] = (160, 160, 160),
    ):
        self.batch_dir = batch_dir
        self.mode = mode
        self.crop_size = crop_size

        # Find all samples
        img_dir = os.path.join(batch_dir, "images")
        lbl_dir = os.path.join(batch_dir, "labels")

        if not os.path.exists(img_dir):
            raise ValueError(f"Image directory not found: {img_dir}")
        if not os.path.exists(lbl_dir):
            raise ValueError(f"Label directory not found: {lbl_dir}")

        img_files = sorted(glob.glob(os.path.join(img_dir, "*.npy")))
        lbl_files = sorted(glob.glob(os.path.join(lbl_dir, "*.npy")))

        if len(img_files) == 0:
            raise ValueError(f"No image files found in {img_dir}")
        if len(lbl_files) == 0:
            raise ValueError(f"No label files found in {lbl_dir}")

        if len(img_files) != len(lbl_files):
            print(f"Warning: {len(img_files)} images but {len(lbl_files)} labels")

        self.sample_ids = [os.path.splitext(os.path.basename(f))[0] for f in img_files]
        self.img_paths = img_files
        self.lbl_paths = lbl_files

        print(f"âœ… Loaded {self.mode} set: {os.path.basename(batch_dir)} ({len(self.sample_ids)} samples)")

    def __len__(self):
        return len(self.sample_ids)

    def __getitem__(self, idx):
        # Load image (memory-mapped, likely float16 from new precompute)
        image = np.load(self.img_paths[idx], mmap_mode='r')

        # Load label (float32, (D, H, W, 6))
        label = np.load(self.lbl_paths[idx])

        D, H, W = image.shape
        cd, ch, cw = self.crop_size

        # Random or center crop
        if self.mode == 'train':
            z = np.random.randint(0, max(1, D - cd + 1))
            y = np.random.randint(0, max(1, H - ch + 1))
            x = np.random.randint(0, max(1, W - cw + 1))
        else:
            z = max(0, (D - cd) // 2)
            y = max(0, (H - ch) // 2)
            x = max(0, (W - cw) // 2)

        # Crop (data is already [0, 1] from precomputation)
        img_crop = image[z:z+cd, y:y+ch, x:x+cw].copy().astype(np.float32)
        lbl_crop = label[z:z+cd, y:y+ch, x:x+cw, :].copy().astype(np.float32)

        if self.mode == 'train':
            gt_mask = lbl_crop[..., 0]

            # Only process if there is ink
            if gt_mask.sum() > 0:
                # CRITICAL FIX: Training labels have porous holes (beta_1 features)
                # that the competition FIXED in the test set using binary_closing((3,3,3)).
                # Training set was NOT fixed, so our model learns to replicate holes.
                # We apply the SAME fix to training labels during loading.
                #
                # Evidence: Label 327851248 has 276 holes in dim 1.
                # After binary_closing((3,3,3)), holes drop to 0.
                # Mean holes across all training: 16.0, with 26 samples having 100+ holes.
                mask_binary = (gt_mask == 1)

                # Step 1: Fill enclosed voids (holes fully surrounded by sheet)
                # binary_fill_holes fills internal cavities without expanding boundaries
                filled = binary_fill_holes(mask_binary)

                # Step 2: Binary closing with full 3D kernel (matches test set fix)
                # 1 iteration fills holes < 3 voxels while preserving sheet boundaries
                structure_3d = np.ones((3, 3, 3), dtype=bool)
                clean_mask = binary_closing(filled, structure=structure_3d, iterations=1)

                # Apply the cleaned mask back
                lbl_crop[..., 0][clean_mask] = 1.0

        # Ensure contiguous
        img_crop = np.ascontiguousarray(img_crop, dtype=np.float32)
        lbl_crop = np.ascontiguousarray(lbl_crop, dtype=np.float32)

        # Convert to tensors
        img_t = torch.from_numpy(img_crop).unsqueeze(0).to(dtype=torch.float32)

        # Extract label channels
        mask_t = torch.from_numpy(lbl_crop[..., 0]).unsqueeze(0).to(dtype=torch.float32)
        skeleton_t = torch.from_numpy(lbl_crop[..., 1]).unsqueeze(0).to(dtype=torch.float32)
        center_t = torch.from_numpy(lbl_crop[..., 2]).unsqueeze(0).to(dtype=torch.float32)
        vectors_t = torch.from_numpy(lbl_crop[..., 3:6]).permute(3, 0, 1, 2).contiguous().to(dtype=torch.float32)

        # Compute valid mask (1 where label != 2)
        raw_label = lbl_crop[..., 0]
        valid_t = torch.from_numpy((raw_label != 2).astype(np.float32)).unsqueeze(-1)

        return {
            'volume': img_t,           # (1, D, H, W) float32 [0, 1]
            'mask': mask_t,            # (1, D, H, W) float32 {0, 1, 2}
            'skeleton': skeleton_t,    # (1, D, H, W) float32 [0, 1]
            'center': center_t,        # (1, D, H, W) float32 [0, 1]
            'vectors': vectors_t,      # (3, D, H, W) float32 normalized
            'valid': valid_t           # (D, H, W, 1) float32 {0, 1}
        }

# ==============================================================================
# MULTI-BATCH DATALOADER CREATION
# ==============================================================================

def create_multi_batch_dataloaders(
    batch_dirs: List[str],
    val_split: float = 0.1,
    batch_size: int = 4,
    num_workers: int = 12,
    crop_size: Tuple[int, int, int] = (160, 160, 160),
) -> Tuple[DataLoader, DataLoader]:
    datasets = []
    for batch_dir in batch_dirs:
        ds = NPYVesuviusDatasetFixed(batch_dir, mode='train', crop_size=crop_size)
        datasets.append(ds)

    combined_dataset = ConcatDataset(datasets)

    num_train = int(len(combined_dataset) * (1 - val_split))
    num_val = len(combined_dataset) - num_train

    train_ds, val_ds = random_split(
        combined_dataset,
        [num_train, num_val],
        generator=torch.Generator().manual_seed(42)
    )

    print(f"âœ… Train samples: {len(train_ds)}, Val samples: {len(val_ds)}")

    train_loader = DataLoader(
        train_ds, batch_size=batch_size, shuffle=True,
        num_workers=num_workers, pin_memory=True, drop_last=True,
        prefetch_factor=3, persistent_workers=True
    )

    val_loader = DataLoader(
        val_ds, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True, drop_last=False,
        prefetch_factor=3, persistent_workers=True
    )

    return train_loader, val_loader

def collate_batch(batch_list):
    volumes = torch.stack([b['volume'] for b in batch_list])
    masks = torch.stack([b['mask'] for b in batch_list])
    skeletons = torch.stack([b['skeleton'] for b in batch_list])
    centers = torch.stack([b['center'] for b in batch_list])
    vectors = torch.stack([b['vectors'] for b in batch_list])
    valids = torch.stack([b['valid'] for b in batch_list])

    return {
        'volume': volumes, 'mask': masks, 'skeleton': skeletons,
        'center': centers, 'vectors': vectors, 'valid': valids
    }

print("âœ… DataLoader Fixed for New Precomputation Format")

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:21:42.244547Z","iopub.execute_input":"2026-02-16T04:21:42.244919Z","iopub.status.idle":"2026-02-16T04:21:42.802806Z","shell.execute_reply.started":"2026-02-16T04:21:42.244895Z","shell.execute_reply":"2026-02-16T04:21:42.802076Z"}}

"""
VESUVIUS VNET INFERENCE WITH SURFACE ENHANCEMENT
=================================================

Features:
âœ… Sliding window inference with Gaussian blending
âœ… 4x Rotation TTA (Test-Time Augmentation)
âœ… Frangi surface filter (surfaceness enhancement)
âœ… 3D Hysteresis thresholding
âœ… Anisotropic morphological closing
âœ… Small object removal
âœ… Submission zip generation

Usage:
    python inference.py --checkpoint best_model.pth
    another model with enhanced epochs and preporcessing steps
    new model trained last epoch 120
"""

import os
import sys
import numpy as np
import torch
import torch.nn.functional as F
import tifffile
import zipfile
import pandas as pd
from tqdm.auto import tqdm
from scipy import ndimage
from skimage.morphology import remove_small_objects

# Import model architecture (assumes vesuvius_vnet.py is in same directory)
from vesuvius_vnet_fixed import VNetFPNFixed, CFG

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class InferenceConfig:
    # Paths
    TEST_DIR = "/kaggle/input/vesuvius-challenge-surface-detection/test_images"
    TEST_CSV = "/kaggle/input/vesuvius-challenge-surface-detection/test.csv"
    OUTPUT_DIR = "/kaggle/working/submission_masks"
    ZIP_PATH = "/kaggle/working/submission.zip"
    
    # Model
    CHECKPOINT_PATH = "/kaggle/input/datasets/perarasu2156/vnet-mamba-holesfilled/gflow_resumed_best_combined_0.5862.pth"
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Inference Settings
    CROP_SIZE = (128, 128, 128)
    OVERLAP = 0.5
    BATCH_SIZE = 2
    
    # Post-processing (Competition-Optimized)
    USE_TTA = False                # 4x rotation TTA
    USE_FRANGI = False           # Surface enhancement filter
    T_LOW = 0.35                 # Hysteresis low threshold
    T_HIGH = 0.90                 # Hysteresis high threshold
    Z_RADIUS = 1                  # Anisotropic closing (z-axis)
    XY_RADIUS = 0                 # Anisotropic closing (xy-plane)
    DUST_MIN_SIZE = 500         # Remove objects smaller than this
    
    # Frangi Filter Parameters
    FRANGI_SCALES = [1.0, 2.0, 3.0]  # Multi-scale surface detection
    FRANGI_ALPHA = 0.5               # Plate-like structure weight
    FRANGI_BETA = 0.5                # Blob-like structure suppression
    FRANGI_GAMMA = 15.0              # Background suppression


# ==============================================================================
# FRANGI SURFACE FILTER (Modified for Sheets, not Vessels)
# ==============================================================================

def frangi_surface_filter_3d(volume, scales=[1.0, 2.0, 3.0], alpha=0.5, beta=0.5, gamma=15.0):
    """
    Modified Frangi filter for SURFACE enhancement (not vessel/tube).
    
    Key difference from standard Frangi:
    - Enhances plate-like structures (2 large eigenvalues, 1 small)
    - Standard Frangi enhances tube-like (1 large, 2 small)
    
    Args:
        volume: (D, H, W) probability map
        scales: Gaussian scales for Hessian computation
        alpha: Plate-likeness sensitivity
        beta: Blob suppression
        gamma: Background suppression (RB threshold)
    
    Returns:
        (D, H, W) surface-enhanced probability map
    """
    volume = volume.astype(np.float32)
    
    # Storage for multi-scale responses
    surface_responses = []
    
    for sigma in scales:
        # Compute Hessian eigenvalues
        # Smooth first
        smoothed = ndimage.gaussian_filter(volume, sigma=sigma)
        
        # Compute second derivatives (Hessian components)
        Dzz = ndimage.gaussian_filter(smoothed, sigma=sigma, order=[2, 0, 0])
        Dyy = ndimage.gaussian_filter(smoothed, sigma=sigma, order=[0, 2, 0])
        Dxx = ndimage.gaussian_filter(smoothed, sigma=sigma, order=[0, 0, 2])
        Dzy = ndimage.gaussian_filter(smoothed, sigma=sigma, order=[1, 1, 0])
        Dzx = ndimage.gaussian_filter(smoothed, sigma=sigma, order=[1, 0, 1])
        Dyx = ndimage.gaussian_filter(smoothed, sigma=sigma, order=[0, 1, 1])
        
        # Build Hessian matrix for each voxel (simplified: use main diagonal approximation)
        # For full accuracy, would need to compute eigenvalues of 3x3 Hessian per voxel
        # Here we use fast approximation: eigenvalues â‰ˆ main diagonal elements
        
        # Sort eigenvalues: |Î»1| â‰¤ |Î»2| â‰¤ |Î»3|
        # For surfaces: Î»1 â‰ˆ 0 (along surface), |Î»2| â‰ˆ |Î»3| >> 0 (perpendicular to surface)
        
        eigvals = np.stack([np.abs(Dzz), np.abs(Dyy), np.abs(Dxx)], axis=0)
        eigvals = np.sort(eigvals, axis=0)
        
        lambda1 = eigvals[0]  # Smallest
        lambda2 = eigvals[1]  # Middle
        lambda3 = eigvals[2]  # Largest
        
        # Avoid division by zero
        lambda2 = np.maximum(lambda2, 1e-10)
        lambda3 = np.maximum(lambda3, 1e-10)
        
        # SURFACE metric (modified Frangi):
        # - Ra: Plate-likeness (Î»1 small, Î»2 â‰ˆ Î»3 large)
        # - Rb: Blob suppression
        # - S: Structural measure
        
        Ra = lambda1 / lambda2  # Should be small for plates
        Rb = lambda2 / lambda3  # Should be close to 1 for plates
        S = np.sqrt(lambda1**2 + lambda2**2 + lambda3**2)  # Frobenius norm
        
        # Surfaceness measure
        surfaceness = (1 - np.exp(-(Ra**2) / (2 * alpha**2))) * \
                      np.exp(-(Rb**2) / (2 * beta**2)) * \
                      (1 - np.exp(-(S**2) / (2 * gamma**2)))
        
        # Only keep dark-to-bright transitions (typical for papyrus sheets)
        surfaceness[lambda3 > 0] = 0
        
        surface_responses.append(surfaceness)
    
    # Take maximum across scales
    final_surface = np.maximum.reduce(surface_responses)
    
    # Normalize to [0, 1]
    if final_surface.max() > 0:
        final_surface = final_surface / final_surface.max()
    
    return final_surface


# ==============================================================================
# ANISOTROPIC MORPHOLOGY HELPERS
# ==============================================================================

def build_anisotropic_struct(z_radius, xy_radius):
    """
    Build anisotropic structuring element for 3D morphology.
    Different radius in z (depth) vs xy (plane).
    """
    if z_radius == 0 and xy_radius == 0:
        return None
    
    if z_radius == 0 and xy_radius > 0:
        # Disk in xy plane only
        size = 2 * xy_radius + 1
        struct = np.zeros((1, size, size), dtype=bool)
        cy, cx = xy_radius, xy_radius
        for dy in range(-xy_radius, xy_radius + 1):
            for dx in range(-xy_radius, xy_radius + 1):
                if dy*dy + dx*dx <= xy_radius*xy_radius:
                    struct[0, cy + dy, cx + dx] = True
        return struct
    
    if z_radius > 0 and xy_radius == 0:
        # Line in z direction only
        struct = np.zeros((2*z_radius + 1, 1, 1), dtype=bool)
        struct[:, 0, 0] = True
        return struct
    
    # Full anisotropic ellipsoid
    depth = 2 * z_radius + 1
    size = 2 * xy_radius + 1
    struct = np.zeros((depth, size, size), dtype=bool)
    cz, cy, cx = z_radius, xy_radius, xy_radius
    
    for dz in range(-z_radius, z_radius + 1):
        for dy in range(-xy_radius, xy_radius + 1):
            for dx in range(-xy_radius, xy_radius + 1):
                if dy*dy + dx*dx <= xy_radius*xy_radius:
                    struct[cz + dz, cy + dy, cx + dx] = True
    
    return struct


# ==============================================================================
# POST-PROCESSING PIPELINE
# ==============================================================================

def topology_aware_postprocess(
    probs,
    use_frangi=True,
    frangi_scales=[1.0, 2.0, 3.0],
    T_low=0.50,
    T_high=0.90,
    z_radius=1,
    xy_radius=0,
    dust_min_size=500,
):
    """
    Competition-optimized post-processing pipeline.
    
    Steps:
    1. Frangi surface enhancement (optional)
    2. 3D Hysteresis thresholding
    3. Anisotropic morphological closing
    4. Small object removal
    
    Args:
        probs: (D, H, W) probability map from model
        use_frangi: Apply Frangi surface filter
        T_low: Hysteresis low threshold
        T_high: Hysteresis high threshold
        z_radius: Closing radius in z-direction
        xy_radius: Closing radius in xy-plane
        dust_min_size: Remove objects smaller than this
    
    Returns:
        (D, H, W) binary mask {0, 1}
    """
    
    # --- Step 1: Frangi Surface Enhancement (OPTIONAL) ---
    if use_frangi:
        print("   Applying Frangi surface filter...")
        surface_enhanced = frangi_surface_filter_3d(
            probs,
            scales=frangi_scales,
            alpha=InferenceConfig.FRANGI_ALPHA,
            beta=InferenceConfig.FRANGI_BETA,
            gamma=InferenceConfig.FRANGI_GAMMA
        )
        # Blend with original probabilities (80% original, 20% surface)
        probs = 0.8 * probs + 0.2 * surface_enhanced
        probs = np.clip(probs, 0, 1)
    
    # --- Step 2: 3D Hysteresis Thresholding ---
    print("   Applying hysteresis thresholding...")
    strong = probs >= T_high
    weak = probs >= T_low
    
    if not strong.any():
        return np.zeros_like(probs, dtype=np.uint8)
    
    struct_hyst = ndimage.generate_binary_structure(3, 3)
    mask = ndimage.binary_propagation(strong, mask=weak, structure=struct_hyst)
    
    if not mask.any():
        return np.zeros_like(probs, dtype=np.uint8)
    
    # --- Step 3: Anisotropic Morphological Closing ---
    if z_radius > 0 or xy_radius > 0:
        print("   Applying anisotropic closing...")
        struct_close = build_anisotropic_struct(z_radius, xy_radius)
        if struct_close is not None:
            mask = ndimage.binary_closing(mask, structure=struct_close)
    
    # --- Step 4: Remove Small Objects (Dust) ---
    if dust_min_size > 0:
        print("   Removing small objects...")
        mask = remove_small_objects(mask.astype(bool), min_size=dust_min_size)
    
    return mask.astype(np.uint8)


# ==============================================================================
# ROTATION TTA HELPERS
# ==============================================================================

def rot90_volume(vol, k):
    """Rotate volume k times 90Â° clockwise in HW plane."""
    if vol.ndim == 5:  # (B, C, D, H, W)
        return torch.rot90(vol, k=-k, dims=(3, 4))
    else:  # (D, H, W)
        return np.rot90(vol, k=-k, axes=(1, 2))


def unrot90_volume(vol, k):
    """Reverse rotation."""
    return rot90_volume(vol, (4 - k) % 4)


# ==============================================================================
# SLIDING WINDOW INFERENCE WITH TTA
# ==============================================================================

@torch.no_grad()
def sliding_window_inference_with_tta(
    model,
    volume,
    crop_size=(160, 160, 160),
    overlap=0.5,
    batch_size=2,
    use_tta=True,
):
    """
    Sliding window inference with optional 4x rotation TTA.
    
    Args:
        model: Trained PyTorch model
        volume: (D, H, W) numpy array
        crop_size: Inference patch size
        overlap: Overlap between patches
        batch_size: Batch size for inference
        use_tta: Apply 4x rotation TTA
    
    Returns:
        (D, H, W) probability map [0, 1]
    """
    model.eval()
    device = InferenceConfig.DEVICE
    
    D, H, W = volume.shape
    cd, ch, cw = crop_size
    
    # Calculate stride
    sd = int(cd * (1 - overlap))
    sh = int(ch * (1 - overlap))
    sw = int(cw * (1 - overlap))
    
    # Generate sliding window coordinates
    z_steps = list(range(0, max(1, D - cd + 1), sd))
    y_steps = list(range(0, max(1, H - ch + 1), sh))
    x_steps = list(range(0, max(1, W - cw + 1), sw))
    
    # Ensure we cover the entire volume
    if z_steps[-1] + cd < D:
        z_steps.append(D - cd)
    if y_steps[-1] + ch < H:
        y_steps.append(H - ch)
    if x_steps[-1] + cw < W:
        x_steps.append(W - cw)
    
    coords = [
        (z, y, x)
        for z in z_steps
        for y in y_steps
        for x in x_steps
    ]
    
    # Rotation TTA configurations (0Â°, 90Â°, 180Â°, 270Â°)
    tta_configs = [0, 1, 2, 3] if use_tta else [0]
    
    # Accumulation buffers
    pred_sum = torch.zeros((D, H, W), dtype=torch.float32, device='cpu')
    count_map = torch.zeros((D, H, W), dtype=torch.float32, device='cpu')
    
    # Gaussian weight map for blending
    gaussian_weight = create_gaussian_weight(crop_size).to(device)
    
    # Process in batches
    for tta_k in tta_configs:
        print(f"   TTA rotation: {tta_k * 90}Â°")
        
        # Rotate volume if needed
        if tta_k > 0:
            vol_rot = rot90_volume(volume, tta_k)
        else:
            vol_rot = volume
        
        for i in tqdm(range(0, len(coords), batch_size), desc=f"  TTA-{tta_k}"):
            batch_coords = coords[i:i+batch_size]
            batch_crops = []
            
            for (z, y, x) in batch_coords:
                crop = vol_rot[z:z+cd, y:y+ch, x:x+cw]
                
                # Pad if needed
                if crop.shape != crop_size:
                    pad_d = cd - crop.shape[0]
                    pad_h = ch - crop.shape[1]
                    pad_w = cw - crop.shape[2]
                    crop = np.pad(crop, ((0, pad_d), (0, pad_h), (0, pad_w)), mode='constant')
                
                batch_crops.append(crop)
            
            # Stack and convert to tensor
            batch_t = torch.from_numpy(np.stack(batch_crops)).unsqueeze(1).float().to(device)
            
            # Model inference
            #with torch.cuda.amp.autocast(enabled=False):
            outputs = model(batch_t)  
                
            if isinstance(outputs, dict):
                logits = outputs['mask']
            elif isinstance(outputs, tuple):
                logits = outputs[0]
            else:
                logits = outputs
                
                # Get probabilities
            probs = torch.sigmoid(logits).squeeze(1)  # (B, D, H, W)
            
            # Apply Gaussian weighting and accumulate
            for j, (z, y, x) in enumerate(batch_coords):
                d_real = min(cd, D - z)
                h_real = min(ch, H - y)
                w_real = min(cw, W - x)
                
                pred_patch = probs[j, :d_real, :h_real, :w_real].cpu()
                weight_patch = gaussian_weight[:d_real, :h_real, :w_real].cpu()
                
                # Rotate back if needed
                if tta_k > 0:
                    pred_patch = unrot90_volume(pred_patch.numpy(), tta_k)
                    pred_patch = torch.from_numpy(pred_patch.copy())
                    # Note: gaussian weight doesn't need rotation as it's symmetric
                
                pred_sum[z:z+d_real, y:y+h_real, x:x+w_real] += pred_patch * weight_patch
                count_map[z:z+d_real, y:y+h_real, x:x+w_real] += weight_patch
        
        # Clear GPU cache
        torch.cuda.empty_cache()
    
    # Average predictions
    pred_avg = pred_sum / torch.clamp(count_map, min=1e-8)
    
    return pred_avg.numpy()


def create_gaussian_weight(size):
    """Create 3D Gaussian weight map for smooth blending."""
    d, h, w = size
    
    # 1D Gaussian
    def gaussian_1d(length, sigma=None):
        if sigma is None:
            sigma = length / 6
        center = length / 2
        x = np.arange(length)
        g = np.exp(-((x - center) ** 2) / (2 * sigma ** 2))
        return g / g.max()
    
    # 3D Gaussian (separable)
    gz = gaussian_1d(d)
    gy = gaussian_1d(h)
    gx = gaussian_1d(w)
    
    weight = np.outer(np.outer(gz, gy).ravel(), gx).reshape(d, h, w)
    
    return torch.from_numpy(weight).float()

def load_compiled_checkpoint(model, ckpt_path, device):
    state = torch.load(ckpt_path, map_location=device)
    if "model_state_dict" in state:
        state = state["model_state_dict"]
    new_state = {}
    for k, v in state.items():
        if k.startswith("_orig_mod."):
            new_state[k.replace("_orig_mod.", "", 1)] = v
        else:
            new_state[k] = v
    model.load_state_dict(new_state, strict=True)
    return model


# ==============================================================================
# MAIN INFERENCE PIPELINE
# ==============================================================================

def predict_volume(
    model,
    volume_path,
    volume_id=None,
):
    """
    Complete inference pipeline for one volume.
    
    Args:
        model: Trained model
        volume_path: Path to .tif volume
        volume_id: Sample ID (for logging)
    
    Returns:
        (D, H, W) binary mask {0, 1}
    """
    
    print(f"\n{'='*60}")
    print(f"Processing: {volume_id if volume_id else volume_path}")
    print(f"{'='*60}")
    
    # Load volume
    print("Loading volume...")
    volume = tifffile.imread(volume_path).astype(np.float32)
    
    # Normalize to [0, 1]
    if volume.max() > 255:
        volume = volume / 65535.0
    elif volume.max() > 1:
        volume = volume / 255.0
    
    print(f"   Shape: {volume.shape}")
    
    # Sliding window inference with TTA
    print("Running inference...")
    probs = sliding_window_inference_with_tta(
        model,
        volume,
        crop_size=InferenceConfig.CROP_SIZE,
        overlap=InferenceConfig.OVERLAP,
        batch_size=InferenceConfig.BATCH_SIZE,
        use_tta=InferenceConfig.USE_TTA,
    )
    
    print(f"   Prob range: [{probs.min():.3f}, {probs.max():.3f}]")
    
    # Post-processing
    print("Post-processing...")
    final_mask = topology_aware_postprocess(
        probs,
        use_frangi=InferenceConfig.USE_FRANGI,
        frangi_scales=InferenceConfig.FRANGI_SCALES,
        T_low=InferenceConfig.T_LOW,
        T_high=InferenceConfig.T_HIGH,
        z_radius=InferenceConfig.Z_RADIUS,
        xy_radius=InferenceConfig.XY_RADIUS,
        dust_min_size=InferenceConfig.DUST_MIN_SIZE,
    )
    
    print(f"   Final mask: {final_mask.sum()} ink voxels")
    
    return final_mask

def generate_gt_and_predictions(num_samples=20):
    print("\n" + "="*80)
    print("VESUVIUS TRAIN INFERENCE - SAVING GT + PRED")
    print("="*80 + "\n")

    TRAIN_IMG_DIR = "/kaggle/input/vesuvius-challenge-surface-detection/train_images"
    TRAIN_LBL_DIR = "/kaggle/input/vesuvius-challenge-surface-detection/train_labels"
    TRAIN_CSV = "/kaggle/input/vesuvius-challenge-surface-detection/train.csv"

    OUT_GT_DIR = "/kaggle/working/gt"
    OUT_PRED_DIR = "/kaggle/working/submission"

    os.makedirs(OUT_GT_DIR, exist_ok=True)
    os.makedirs(OUT_PRED_DIR, exist_ok=True)

    df = pd.read_csv(TRAIN_CSV)
    df['id'] = df['id'].astype(str)

    sample_ids = df.sample(num_samples, random_state=42)['id'].tolist()
    print("Using samples:", sample_ids)

    print("\nLoading model...")
    model = VNetFPNFixed().to(InferenceConfig.DEVICE)
    model = load_compiled_checkpoint(
        model,
        InferenceConfig.CHECKPOINT_PATH,
        InferenceConfig.DEVICE
    )
    model.eval()
    print("Model loaded\n")

    for idx, sample_id in enumerate(sample_ids):
        print(f"\n[{idx+1}/{len(sample_ids)}] Processing {sample_id}")

        img_path = os.path.join(TRAIN_IMG_DIR, f"{sample_id}.tif")
        lbl_path = os.path.join(TRAIN_LBL_DIR, f"{sample_id}.tif")

        volume = tifffile.imread(img_path).astype(np.float32)

        if volume.max() > 255:
            volume = volume / 65535.0
        elif volume.max() > 1:
            volume = volume / 255.0

        probs = sliding_window_inference_with_tta(
            model,
            volume,
            crop_size=InferenceConfig.CROP_SIZE,
            overlap=InferenceConfig.OVERLAP,
            batch_size=InferenceConfig.BATCH_SIZE,
            use_tta=InferenceConfig.USE_TTA,
        )

        pred_mask = topology_aware_postprocess(
            probs,
            use_frangi=InferenceConfig.USE_FRANGI,
            frangi_scales=InferenceConfig.FRANGI_SCALES,
            T_low=InferenceConfig.T_LOW,
            T_high=InferenceConfig.T_HIGH,
            z_radius=InferenceConfig.Z_RADIUS,
            xy_radius=InferenceConfig.XY_RADIUS,
            dust_min_size=InferenceConfig.DUST_MIN_SIZE,
        )

        gt_mask = tifffile.imread(lbl_path)
        gt_mask = (gt_mask == 1).astype(np.uint8)

        out_pred_path = os.path.join(OUT_PRED_DIR, f"{sample_id}.tif")
        out_gt_path = os.path.join(OUT_GT_DIR, f"{sample_id}.tif")

        tifffile.imwrite(out_pred_path, pred_mask.astype(np.uint8))
        tifffile.imwrite(out_gt_path, gt_mask.astype(np.uint8))

        print("Saved:")
        print("  PRED ->", out_pred_path)
        print("  GT   ->", out_gt_path)

    print("\n" + "="*80)
    print("DONE. Files ready for metric.")
    print("GT DIR:", OUT_GT_DIR)
    print("PRED DIR:", OUT_PRED_DIR)
    print("="*80)


# ==============================================================================
# SUBMISSION GENERATION
# ==============================================================================

def generate_submission():
    """
    Generate submission zip file.
    """
    
    print("\n" + "="*80)
    print("VESUVIUS VNET INFERENCE - GENERATING SUBMISSION")
    print("="*80 + "\n")
    
    # Load test metadata
    test_df = pd.read_csv(InferenceConfig.TEST_CSV)
    print(f"Test samples: {len(test_df)}\n")
    
    # Create output directory
    os.makedirs(InferenceConfig.OUTPUT_DIR, exist_ok=True)
    
    # Load model
    print("Loading model...")
    model = VNetFPNFixed()

    # âœ… FIXED: load compiled checkpoint correctly
    model = load_compiled_checkpoint(
        model,
        InferenceConfig.CHECKPOINT_PATH,
        InferenceConfig.DEVICE
    )
    if torch.cuda.device_count() > 1:
        print("Using", torch.cuda.device_count(), "GPUs")
        model = torch.nn.DataParallel(model)

    model = model.to(InferenceConfig.DEVICE)
    model.eval()
    print(f"âœ… Model loaded from {InferenceConfig.CHECKPOINT_PATH}\n")

    print(f"âœ… Model loaded from {InferenceConfig.CHECKPOINT_PATH}\n")
    
    # Process each test volume
    with zipfile.ZipFile(InferenceConfig.ZIP_PATH, 'w', compression=zipfile.ZIP_DEFLATED) as z:
        for idx, row in test_df.iterrows():
            sample_id = str(row['id'])
            volume_path = os.path.join(InferenceConfig.TEST_DIR, f"{sample_id}.tif")
            
            if not os.path.exists(volume_path):
                print(f"âš ï¸  Volume not found: {volume_path}")
                continue
            
            # Predict
            mask = predict_volume(model, volume_path, volume_id=sample_id)
            
            # Save
            output_path = os.path.join(InferenceConfig.OUTPUT_DIR, f"{sample_id}.tif")
            tifffile.imwrite(output_path, mask.astype(np.uint8))
            
            # Add to zip
            z.write(output_path, arcname=f"{sample_id}.tif")
            
            # Clean up
            os.remove(output_path)
            
            print(f"âœ… Added to submission: {sample_id}.tif\n")
    
    print("\n" + "="*80)
    print(f"âœ… SUBMISSION COMPLETE: {InferenceConfig.ZIP_PATH}")
    print("="*80)



# %% [code] {"execution":{"iopub.status.busy":"2026-02-16T04:24:15.661094Z","iopub.execute_input":"2026-02-16T04:24:15.661647Z","iopub.status.idle":"2026-02-16T04:24:16.791515Z","shell.execute_reply.started":"2026-02-16T04:24:15.661615Z","shell.execute_reply":"2026-02-16T04:24:16.790722Z"}}
import os
os.environ["KERAS_BACKEND"] = "tensorflow"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

# ============================================================================
# SETUP PACKAGES
# ============================================================================
var = "/kaggle/input/vsdetection-packages-offline-installer-only/whls"
if os.path.exists(var):
    print(f"Installing packages from: {var}")
    import subprocess
    subprocess.run([
        "pip", "install", "--quiet",
        f"{var}/keras_nightly-3.12.0.dev2025100703-py3-none-any.whl",
        f"{var}/tifffile-2025.10.16-py3-none-any.whl",
        f"{var}/imagecodecs-2025.11.11-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl",
        f"{var}/medicai-0.0.3-py3-none-any.whl",
        "--no-index",
        "--find-links", var
    ], check=False, capture_output=True)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:24:26.770882Z","iopub.execute_input":"2026-02-16T04:24:26.771183Z","iopub.status.idle":"2026-02-16T04:24:26.791226Z","shell.execute_reply.started":"2026-02-16T04:24:26.771155Z","shell.execute_reply":"2026-02-16T04:24:26.790374Z"}}

"""
================================================================================
   VESUVIUS V19 - THE REFINED CHAMPION

   - Base: V16 (Best Score: 0.533)
   - Overlap: 0.25 (Proven best for sharpness)
   - Thresholds: 0.85 / 0.45 (Proven best balance)
   - Optimization: LOGIT Averaging (Mathematically superior to Prob averaging)
================================================================================
"""



import keras
from keras import ops
from medicai.transforms import Compose, ScaleIntensityRange
from medicai.models import TransUNet
from medicai.utils.inference import SlidingWindowInference

import numpy as np
import pandas as pd
import zipfile
import tifffile
import scipy.ndimage as ndi
from skimage.morphology import remove_small_objects
import gc

print("="*60)
print("VESUVIUS V19 - LOGIT ENSEMBLE + V16 SETTINGS")
print("="*60)

# ============================================================================
# CONFIG
# ============================================================================
root_dir = "/kaggle/input/vesuvius-challenge-surface-detection"
test_dir = f"{root_dir}/test_images"
output_dir = "/kaggle/working/submission_masks"
zip_path = "/kaggle/working/submission.zip"
os.makedirs(output_dir, exist_ok=True)

# Model config
NUM_CLASSES = 3
PATCH_SIZE = (160, 160, 160)
OVERLAP = 0.25  # STRICTLY 0.25 (The V16 Winner)

# Post-processing config (V16 Winner Settings)
T_LOW = 0.45    # Permissive connectivity
T_HIGH = 0.85   # High confidence seeds
Z_RADIUS = 1
XY_RADIUS = 0
DUST_MIN_SIZE = 500

# ============================================================================
# MODEL LOADING (LOGIT MODE)
# ============================================================================
def get_ensemble_models():
    # 1. Baseline Model (Weight: 0.25)
    path1 = "/kaggle/input/train-transunet-baseline-lb-0-537/fine_tuning_epoch_20.weights.h5"
    # 2. Stronger Model (Weight: 0.75)
    path2 = "/kaggle/input/train-vesuvius-surface-3d-detection-on-tpu/model.weights.h5"
    
    models = []
    
    # classifier_activation=None -> Output Logits
    if os.path.exists(path1):
        print(f"Loading Model 1: {path1}")
        m1 = TransUNet(input_shape=(160, 160, 160, 1), encoder_name='seresnext50', classifier_activation=None, num_classes=NUM_CLASSES)
        m1.load_weights(path1)
        models.append(m1)
    
    if os.path.exists(path2):
        print(f"Loading Model 2: {path2}")
        m2 = TransUNet(input_shape=(160, 160, 160, 1), encoder_name='seresnext50', classifier_activation=None, num_classes=NUM_CLASSES)
        m2.load_weights(path2)
        models.append(m2)
        
    if not models:
        print("WARNING: No weights found. Initializing random model.")
        m = TransUNet(input_shape=(160, 160, 160, 1), encoder_name='seresnext50', classifier_activation=None, num_classes=NUM_CLASSES)
        return [m]
        
    return models

# ============================================================================
# TRANSFORMS & HELPERS
# ============================================================================
def val_transformation(image):
    data = {"image": image}
    pipeline = Compose([ScaleIntensityRange(keys=["image"], a_min=0, a_max=255, b_min=0, b_max=1, clip=True)])
    result = pipeline(data)
    return result["image"]

def load_volume(path):
    vol = tifffile.imread(path).astype(np.float32)
    vol = vol[None, ..., None]
    return vol

# ============================================================================
# INFERENCE LOGIC (4x TTA + LOGITS)
# ============================================================================
def predict_with_tta(inputs, swi):
    """4x TTA (Rotations only) - Returns LOGITS."""
    logits = []
    # Original
    logits.append(swi(inputs))
    # Rotations
    for k in [1, 2, 3]:
        img_r = np.rot90(inputs, k=k, axes=(2, 3))
        p = swi(img_r)
        p = np.rot90(p, k=-k, axes=(2, 3))
        logits.append(p)
    return np.mean(logits, axis=0)

def ensemble_predict(inputs, models):
    # Weighted Ensemble
    weights = [0.25, 0.75] if len(models) == 2 else [1.0/len(models)]*len(models)
    ensemble_logits = []
    
    for i, model in enumerate(models):
        swi = SlidingWindowInference(model, num_classes=NUM_CLASSES, roi_size=PATCH_SIZE, sw_batch_size=1, mode='gaussian', overlap=OVERLAP)
        
        # Get Logits
        logits = predict_with_tta(inputs, swi)
        ensemble_logits.append(logits * weights[i])
    
    # Sum weighted logits
    total_logits = np.sum(ensemble_logits, axis=0)
    
    # Softmax to get Probabilities
    probs = ops.softmax(total_logits, axis=-1)
    
    # Return Foreground Probability (Class 1)
    return np.squeeze(probs[..., 1])

# ============================================================================
# POST-PROCESSING (HYSTERESIS)
# ============================================================================
def build_anisotropic_struct(z_radius, xy_radius):
    z, r = z_radius, xy_radius
    if z == 0 and r == 0: return None
    depth = 2 * z + 1
    size = 2 * r + 1
    struct = np.zeros((depth, size, size), dtype=bool)
    cz, cy, cx = z, r, r
    for dz in range(-z, z + 1):
        for dy in range(-r, r + 1):
            for dx in range(-r, r + 1):
                if dy * dy + dx * dx <= r * r:
                    struct[cz + dz, cy + dy, cx + dx] = True
    return struct

def topo_postprocess(probs, T_low, T_high, z_radius, xy_radius, dust_min_size):
    # 1. Hysteresis Thresholding
    strong = probs >= T_high
    weak = probs >= T_low
    
    if not strong.any(): return np.zeros_like(probs, dtype=np.uint8)
    
    struct_hyst = ndi.generate_binary_structure(3, 3)
    mask = ndi.binary_propagation(strong, mask=weak, structure=struct_hyst)
    
    if not mask.any(): return np.zeros_like(probs, dtype=np.uint8)

    # 2. Anisotropic Closing
    struct = build_anisotropic_struct(z_radius, xy_radius)
    if struct is not None:
        mask = ndi.binary_closing(mask, structure=struct)

    # 3. Dust Removal
    if dust_min_size > 0:
        mask = remove_small_objects(mask.astype(bool), min_size=dust_min_size)

    return mask.astype(np.uint8)

# ============================================================================
# MAIN PIPELINE
# ============================================================================
print("\nLoading models...")
models = get_ensemble_models()
test_df = pd.read_csv(f"{root_dir}/test.csv")

print("\nStarting Inference...")
with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as z:
    for idx, row in test_df.iterrows():
        image_id = row["id"]
        print(f"\n[{idx+1}/{len(test_df)}] Processing {image_id}...")
        
        # 1. Load
        vol = load_volume(f"{test_dir}/{image_id}.tif")
        vol = val_transformation(vol)
        
        # 2. Predict (Ensemble -> 4x TTA -> Logits -> Softmax)
        probs = ensemble_predict(vol, models)
        
        # 3. Post-process (Hysteresis)
        final_mask = topo_postprocess(
            probs, 
            T_low=T_LOW, 
            T_high=T_HIGH, 
            z_radius=Z_RADIUS, 
            xy_radius=XY_RADIUS, 
            dust_min_size=DUST_MIN_SIZE
        )
        
        # 4. Save
        print(f"    Foreground voxels: {final_mask.sum():,}")
        out_path = f"{output_dir}/{image_id}.tif"
        tifffile.imwrite(out_path, final_mask)
        z.write(out_path, arcname=f"{image_id}.tif")
        os.remove(out_path)
        
        del vol, probs, final_mask
        gc.collect()

print("\n" + "="*60)
print("V19 COMPLETE")
print(f"Submission: {zip_path}")
print("="*60)

# %% [code] {"execution":{"iopub.status.busy":"2026-02-16T04:24:41.769621Z","iopub.execute_input":"2026-02-16T04:24:41.769944Z","iopub.status.idle":"2026-02-16T04:24:41.780180Z","shell.execute_reply.started":"2026-02-16T04:24:41.769914Z","shell.execute_reply":"2026-02-16T04:24:41.779375Z"}}
import os

# --- CRITICAL FIX: PREVENT TENSORFLOW FROM HOGGING GPU ---
os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"
os.environ["KERAS_BACKEND"] = "tensorflow"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True" # Helps fragmentation

# Now safe to import
import torch
import numpy as np
import tifffile
import glob
import gc
import zipfile
import pandas as pd
from keras import ops
import keras
import medicai
from medicai.transforms import Compose, ScaleIntensityRange
from medicai.models import TransUNet
from medicai.utils.inference import SlidingWindowInference

# Re-define your PyTorch Loader to be safer (CPU -> GPU)
def load_compiled_checkpoint(model, ckpt_path, device):
    print(f"  Loading checkpoint to CPU first...")
    # Load to CPU first to avoid VRAM spike
    state = torch.load(ckpt_path, map_location="cpu")
    if "model_state_dict" in state:
        state = state["model_state_dict"]
    
    # Fix keys
    new_state = {}
    for k, v in state.items():
        if k.startswith("_orig_mod."):
            new_state[k.replace("_orig_mod.", "", 1)] = v
        else:
            new_state[k] = v
            
    model.load_state_dict(new_state, strict=True)
    print(f"  Moving model to {device}...")
    return model.to(device)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:22:03.680580Z","iopub.status.idle":"2026-02-16T04:22:03.680938Z","shell.execute_reply.started":"2026-02-16T04:22:03.680759Z","shell.execute_reply":"2026-02-16T04:22:03.680782Z"}}
# # import zipfile, tifffile, numpy as np, matplotlib.pyplot as plt

# # zip_path = "/kaggle/working/submission.zip"

# # with zipfile.ZipFile(zip_path, 'r') as z:
# #     names = z.namelist()
# #     print("files:", names)
# #     name = names[0]
# #     with z.open(name) as f:
# #         mask = tifffile.imread(f)

# # print("mask shape:", mask.shape)
# # print("ink voxels:", int(mask.sum()))

# # cz = mask.shape[0] // 2
# # plt.imshow(mask[cz], cmap='gray')
# # plt.title("axial slice")
# # plt.axis('off')
# # plt.show()

# # plt.imshow(mask.max(axis=0), cmap='gray')
# # plt.title("MIP over Z")
# # plt.axis('off')
# # plt.show()


# import zipfile, tifffile, numpy as np, matplotlib.pyplot as plt
# import ipywidgets as widgets
# from IPython.display import display

# zip_path = "/kaggle/working/submission.zip"

# with zipfile.ZipFile(zip_path, 'r') as z:
#     names = z.namelist()
#     print("files:", names)
#     name = names[0]
#     with z.open(name) as f:
#         mask = tifffile.imread(f)

# print("mask shape:", mask.shape)
# print("ink voxels:", int(mask.sum()))

# slider = widgets.IntSlider(min=0, max=mask.shape[0]-1, step=1)

# def view_slice(z):
#     plt.imshow(mask[z], cmap='gray')
#     plt.title(f"slice {z}")
#     plt.axis('off')
#     plt.show()

# display(slider)
# widgets.interact(view_slice, z=slider)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:22:03.681944Z","iopub.status.idle":"2026-02-16T04:22:03.682246Z","shell.execute_reply.started":"2026-02-16T04:22:03.682107Z","shell.execute_reply":"2026-02-16T04:22:03.682140Z"}}
# from kaggle_secrets import UserSecretsClient
# from huggingface_hub import login, create_repo, HfApi

# # Load HF token from Kaggle secrets
# user_secrets = UserSecretsClient()
# HF_TOKEN = user_secrets.get_secret("HF_TOKEN")

# # Login
# login(token=HF_TOKEN)

# # Repo info
# REPO_ID = "ragunath-ravi/vesuvius-gt-vs-pred"

# # Create dataset repo (ignore error if already exists)
# try:
#     create_repo(
#         repo_id=REPO_ID,
#         repo_type="dataset",
#         private=False
#     )
# except:
#     pass

# # Upload folders
# api = HfApi()
# api.upload_folder(
#     folder_path="/kaggle/working",
#     repo_id=REPO_ID,
#     repo_type="dataset",
#     path_in_repo="."
# )

# print("UPLOAD COMPLETE:", REPO_ID)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-02-16T04:22:03.683198Z","iopub.status.idle":"2026-02-16T04:22:03.683457Z","shell.execute_reply.started":"2026-02-16T04:22:03.683326Z","shell.execute_reply":"2026-02-16T04:22:03.683350Z"}}
!
